{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get distances clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UTILS_files.py imported successfully\n"
     ]
    }
   ],
   "source": [
    "import TRACE_module.preprocessing as pp\n",
    "\n",
    "import TRACE_module.visualisation as vi\n",
    "from TRACE_module.descriptive_analysis import from_stack_to_number_of_interaction_sequences,from_distance_to_sequences_vector, from_distances_to_sequences_stack,from_seq_to_average_interaction_time,from_seq_to_daily_interactions\n",
    "from TRACE_module.apriori_spade import stack_to_one_hot_df, get_maximum_connex_graph, apriori_\n",
    "from TRACE_module.DTGC import create_file_x_y_t as create_file_x_y_t\n",
    "from TRACE_module.DTGC import create_file_nodeId_label as create_file_nodeId_label\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import dotenv\n",
    "import sys\n",
    "import time\n",
    "from TRACE_module.env_loading import parent_dir, output_dir\n",
    "\n",
    "if os.getcwd().endswith(\"TRACE_module\"):\n",
    "    new_path = os.getcwd()[:-len(\"TRACE_module\")] + \"UTILS_module\"\n",
    "elif os.getcwd().endswith(\"TRACE_IODAA\"):\n",
    "    new_path = os.getcwd() + os.sep + \"UTILS_module\"\n",
    "sys.path.insert(0, new_path) # insert path to be able to import file afterwards, allows for imports wherever the script is called from\n",
    "\n",
    "from UTILS_files import test_import\n",
    "if test_import() : # Testing the import\n",
    "    from UTILS_files import get_all_files_within_one_folder # Importing the wanted function\n",
    "else :\n",
    "    print(\"Import failed\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder= parent_dir\n",
    "list_files=list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to get all files :  0.0024556669959565625\n",
      "['/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_365d_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_365e_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3660_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3662_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3663_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3666_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3667_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3668_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3669_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_366a_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_366c_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_366d_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3ce9_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cea_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3ceb_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cec_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3ced_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cee_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cef_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf0_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf1_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf2_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf3_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf4_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf5_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf6_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf7_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf8_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cf9_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cfa_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cfb_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cfc_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cfd_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cfe_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3cff_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d01_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d02_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d03_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d04_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d05_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d06_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d07_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d08_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d09_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d0c_glob_sensor_time_ble.parquet', '/Users/josephallyndree/Dropbox/Joseph/A-PhD/PFR INRAE - Graphes reseaux/20241029 - Buisson/Data_rssi_glob_sensor_time/20241016-20241029_3d0f_glob_sensor_time_ble.parquet']\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "list_files = get_all_files_within_one_folder(folder, True, extension=\".parquet\")\n",
    "\n",
    "print(list_files)\n",
    "print(len(list_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(folder_savings) :\n\u001b[1;32m      3\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(folder_savings)\n\u001b[0;32m----> 5\u001b[0m data\u001b[38;5;241m=\u001b[39m\u001b[43mpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlist_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43msmooth_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m20s\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m data\u001b[38;5;241m=\u001b[39mpp\u001b[38;5;241m.\u001b[39mremove_captor_(data,[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m366b\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      8\u001b[0m pprint(data)\n",
      "File \u001b[0;32m~/Dropbox/Joseph/A-PhD/TRACE_IODAA/TRACE_module/preprocessing.py:90\u001b[0m, in \u001b[0;36mconcatenate_df\u001b[0;34m(liste_files, smooth_time)\u001b[0m\n\u001b[1;32m     88\u001b[0m concatenated_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame()\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m liste_files:\n\u001b[0;32m---> 90\u001b[0m     concatenated_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     91\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43mconcatenated_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mformat_df_to_concatenation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msmooth_time\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msmooth_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m concatenated_df\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/pandas/core/reshape/concat.py:395\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[0;32m--> 395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/pandas/core/reshape/concat.py:684\u001b[0m, in \u001b[0;36m_Concatenator.get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    680\u001b[0m             indexers[ax] \u001b[38;5;241m=\u001b[39m obj_labels\u001b[38;5;241m.\u001b[39mget_indexer(new_labels)\n\u001b[1;32m    682\u001b[0m     mgrs_indexers\u001b[38;5;241m.\u001b[39mappend((obj\u001b[38;5;241m.\u001b[39m_mgr, indexers))\n\u001b[0;32m--> 684\u001b[0m new_data \u001b[38;5;241m=\u001b[39m \u001b[43mconcatenate_managers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmgrs_indexers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_axes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcat_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm_axis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\n\u001b[1;32m    686\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    688\u001b[0m     new_data\u001b[38;5;241m.\u001b[39m_consolidate_inplace()\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/pandas/core/internals/concat.py:177\u001b[0m, in \u001b[0;36mconcatenate_managers\u001b[0;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[1;32m    167\u001b[0m vals \u001b[38;5;241m=\u001b[39m [ju\u001b[38;5;241m.\u001b[39mblock\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m ju \u001b[38;5;129;01min\u001b[39;00m join_units]\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_extension:\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# _is_uniform_join_units ensures a single dtype, so\u001b[39;00m\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;66;03m#  we can use np.concatenate, which is more performant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;66;03m# expected \"Union[_SupportsArray[dtype[Any]],\u001b[39;00m\n\u001b[1;32m    176\u001b[0m     \u001b[38;5;66;03m# _NestedSequence[_SupportsArray[dtype[Any]]]]\"\u001b[39;00m\n\u001b[0;32m--> 177\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_1d_only_ea_dtype(blk\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# TODO(EA2D): special-casing not needed with 2D EAs\u001b[39;00m\n\u001b[1;32m    180\u001b[0m     values \u001b[38;5;241m=\u001b[39m concat_compat(vals, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, ea_compat_axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "folder_savings =  os.sep.join([output_dir,\"savings\"])\n",
    "if not os.path.isdir(folder_savings) :\n",
    "    os.makedirs(folder_savings)\n",
    "\n",
    "data=pp.concatenate_df(list_files,smooth_time='20s')\n",
    "data=pp.remove_captor_(data,[\"366b\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       glob_sensor_DateTime accelero_id  RSSI id_sensor\n",
      "0       2024-10-15 14:16:00        365e -61.0      365d\n",
      "1       2024-10-15 14:16:20        365e -73.0      365d\n",
      "2       2024-10-15 14:16:40        365e -78.0      365d\n",
      "3       2024-10-15 14:17:00        365e -62.0      365d\n",
      "4       2024-10-15 14:17:20        365e -50.0      365d\n",
      "...                     ...         ...   ...       ...\n",
      "436722  2024-10-29 17:29:40        3d0c -59.0      3d0f\n",
      "436723  2024-10-29 17:30:00        3d0c -61.0      3d0f\n",
      "436724  2024-10-29 17:30:20        3d0c -59.0      3d0f\n",
      "436725  2024-10-29 17:30:40        3d0c -57.0      3d0f\n",
      "436726  2024-10-29 17:31:00        3d0c -55.0      3d0f\n",
      "\n",
      "[25272458 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       glob_sensor_DateTime accelero_id  RSSI id_sensor  evaluated_distance\n",
      "0       2024-10-15 14:16:00        365e -61.0      365d          107.977516\n",
      "1       2024-10-15 14:16:20        365e -73.0      365d          271.227258\n",
      "2       2024-10-15 14:16:40        365e -78.0      365d          398.107171\n",
      "3       2024-10-15 14:17:00        365e -62.0      365d          116.591440\n",
      "4       2024-10-15 14:17:20        365e -50.0      365d           46.415888\n",
      "...                     ...         ...   ...       ...                 ...\n",
      "436722  2024-10-29 17:29:40        3d0c -59.0      3d0f           92.611873\n",
      "436723  2024-10-29 17:30:00        3d0c -61.0      3d0f          107.977516\n",
      "436724  2024-10-29 17:30:20        3d0c -59.0      3d0f           92.611873\n",
      "436725  2024-10-29 17:30:40        3d0c -57.0      3d0f           79.432823\n",
      "436726  2024-10-29 17:31:00        3d0c -55.0      3d0f           68.129207\n",
      "\n",
      "[25272458 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "data=pp.transform_rssi_to_distance(data)\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['2024-10-15T14:16:00.000000000', '2024-10-15T14:16:20.000000000',\n",
       "       '2024-10-15T14:16:40.000000000', ...,\n",
       "       '2024-10-15T12:45:40.000000000', '2024-10-15T12:46:00.000000000',\n",
       "       '2024-10-15T12:46:20.000000000'],\n",
       "      shape=(61654,), dtype='datetime64[ns]')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_id=list(pd.unique(data[\"accelero_id\"]))\n",
    "\n",
    "stack,list_timesteps=pp.create_stack(data,list_id)\n",
    "\n",
    "# print(stack)\n",
    "list_timesteps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  ...\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]\n",
      "\n",
      " [[0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 1 0 0]\n",
      "  ...\n",
      "  [0 0 1 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]\n",
      "  [0 0 0 ... 0 0 0]]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "###BUISSON\n",
    "start_time = pd.Timestamp('2024-10-18T15:00:00.000000000')\n",
    "end_time = pd.Timestamp('2024-10-27T06:00:00.000000000')\n",
    "\n",
    "distances_clean,list_timesteps=pp.crop_start_end_stack(stack=stack,\n",
    "                                                        list_timesteps = list_timesteps ,\n",
    "                                                        start = start_time,\n",
    "                                                        end = end_time)\n",
    "\n",
    "print(distances_clean)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Starting HTNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from TRACE_module.DTGC import convert_HTNE_embFile, TSNE_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.80000000e+01 3.80000000e+01 1.87869028e-04]\n",
      " [2.10000000e+01 3.30000000e+01 1.87869028e-04]\n",
      " [3.10000000e+01 3.40000000e+01 1.87869028e-04]\n",
      " ...\n",
      " [1.60000000e+01 2.90000000e+01 3.57326892e-01]\n",
      " [1.80000000e+01 3.50000000e+01 3.57326892e-01]\n",
      " [2.00000000e+01 3.10000000e+01 3.57326892e-01]]\n"
     ]
    }
   ],
   "source": [
    "# Loading cow.txt\n",
    "path_cow_data = \"/Users/josephallyndree/Dropbox/Joseph/A-PhD/TRACE_IODAA/cows.txt\"\n",
    "\n",
    "converted_data = convert_HTNE_embFile(path_cow_data, 1000000)\n",
    "\n",
    "print(converted_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS is available.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch.optim import SGD\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.functional import softmax\n",
    "import sys\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "FType = torch.FloatTensor\n",
    "LType = torch.LongTensor\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print(\"MPS is available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class HTNEDataSet(Dataset):\n",
    "    def __init__(self, file_path, neg_size, hist_len, directed=False, transform=None):\n",
    "        self.neg_size = neg_size\n",
    "        self.hist_len = hist_len\n",
    "        self.directed = directed\n",
    "        self.transform = transform\n",
    "\n",
    "        # self.max_d_time = -sys.maxint  # Time interval [0, T]\n",
    "        self.max_d_time = -sys.maxsize\n",
    "        self.NEG_SAMPLING_POWER = 0.75\n",
    "        self.neg_table_size = int(1e8)\n",
    "\n",
    "        self.node2hist = dict()\n",
    "        self.node_set = set()\n",
    "        self.degrees = dict()\n",
    "        with open(file_path, 'r') as infile:\n",
    "            for line in infile:\n",
    "                parts = line.split()\n",
    "                s_node = int(parts[0])  # source node\n",
    "                t_node = int(parts[1])  # target node\n",
    "                d_time = float(parts[2])  # time slot, delta t\n",
    "\n",
    "                self.node_set.update([s_node, t_node])\n",
    "\n",
    "                if s_node not in self.node2hist:\n",
    "                    self.node2hist[s_node] = list()\n",
    "                self.node2hist[s_node].append((t_node, d_time))\n",
    "\n",
    "                if not directed:\n",
    "                    if t_node not in self.node2hist:\n",
    "                        self.node2hist[t_node] = list()\n",
    "                    self.node2hist[t_node].append((s_node, d_time))\n",
    "\n",
    "                if d_time > self.max_d_time:\n",
    "                    self.max_d_time = d_time\n",
    "\n",
    "                if s_node not in self.degrees:\n",
    "                    self.degrees[s_node] = 0\n",
    "                if t_node not in self.degrees:\n",
    "                    self.degrees[t_node] = 0\n",
    "                self.degrees[s_node] += 1\n",
    "                self.degrees[t_node] += 1\n",
    "\n",
    "        self.node_dim = len(self.node_set)\n",
    "\n",
    "        self.data_size = 0\n",
    "        for s in self.node2hist:\n",
    "            hist = self.node2hist[s]\n",
    "            hist = sorted(hist, key=lambda x: x[1])\n",
    "            self.node2hist[s] = hist\n",
    "            self.data_size += len(self.node2hist[s])\n",
    "\n",
    "        self.idx2source_id = np.zeros((self.data_size,), dtype=np.int32)\n",
    "        self.idx2target_id = np.zeros((self.data_size,), dtype=np.int32)\n",
    "        idx = 0\n",
    "        for s_node in self.node2hist:\n",
    "            for t_idx in range(len(self.node2hist[s_node])):\n",
    "                self.idx2source_id[idx] = s_node\n",
    "                self.idx2target_id[idx] = t_idx\n",
    "                idx += 1\n",
    "\n",
    "        self.neg_table = np.zeros((self.neg_table_size,))\n",
    "        self.init_neg_table()\n",
    "\n",
    "    def get_node_dim(self):\n",
    "        return self.node_dim\n",
    "\n",
    "    def get_max_d_time(self):\n",
    "        return self.max_d_time\n",
    "\n",
    "    def init_neg_table(self):\n",
    "        tot_sum, cur_sum, por = 0., 0., 0.\n",
    "        n_id = 0\n",
    "        print(self.node_dim)\n",
    "        for k in range(self.node_dim):\n",
    "            tot_sum += np.power(self.degrees[k], self.NEG_SAMPLING_POWER)\n",
    "        for k in range(self.neg_table_size):\n",
    "            if (k + 1.) / self.neg_table_size > por:\n",
    "                cur_sum += np.power(self.degrees[n_id], self.NEG_SAMPLING_POWER)\n",
    "                por = cur_sum / tot_sum\n",
    "                n_id += 1\n",
    "            self.neg_table[k] = n_id - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        s_node = self.idx2source_id[idx]\n",
    "        t_idx = self.idx2target_id[idx]\n",
    "        t_node = self.node2hist[s_node][t_idx][0]\n",
    "        t_time = self.node2hist[s_node][t_idx][1]\n",
    "\n",
    "        if t_idx - self.hist_len < 0:\n",
    "            hist = self.node2hist[s_node][0:t_idx]\n",
    "        else:\n",
    "            hist = self.node2hist[s_node][t_idx - self.hist_len:t_idx]\n",
    "        \n",
    "        hist_nodes = [h[0] for h in hist]\n",
    "        hist_times = [h[1] for h in hist]\n",
    "\n",
    "        np_h_nodes = np.zeros((self.hist_len,))\n",
    "        np_h_nodes[:len(hist_nodes)] = hist_nodes\n",
    "        np_h_times = np.zeros((self.hist_len,))\n",
    "        np_h_times[:len(hist_times)] = hist_times\n",
    "        np_h_masks = np.zeros((self.hist_len,))\n",
    "        np_h_masks[:len(hist_nodes)] = 1.\n",
    "\n",
    "        neg_nodes = self.negative_sampling()\n",
    "        \n",
    "        sample = {\n",
    "            'source_node': s_node,\n",
    "            'target_node': t_node,\n",
    "            'target_time': t_time,\n",
    "            'history_nodes': np_h_nodes,\n",
    "            'history_times': np_h_times,\n",
    "            'history_masks': np_h_masks,\n",
    "            'neg_nodes': neg_nodes,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def negative_sampling(self):\n",
    "        rand_idx = np.random.randint(0, self.neg_table_size, (self.neg_size,))\n",
    "        sampled_nodes = self.neg_table[rand_idx]\n",
    "        return sampled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HTNE_a:\n",
    "    def __init__(self, file_path, emb_size=128, neg_size=10, hist_len=2, directed=False,\n",
    "            learning_rate=0.01, batch_size=1000, save_step=50, epoch_num=15):\n",
    "        self.emb_size = emb_size\n",
    "        self.neg_size = neg_size\n",
    "        self.hist_len = hist_len\n",
    "\n",
    "        self.lr = learning_rate\n",
    "        self.batch = batch_size\n",
    "        self.save_step = save_step\n",
    "        self.epochs = epoch_num\n",
    "\n",
    "        self.data = HTNEDataSet(file_path, neg_size, hist_len, directed)\n",
    "        self.node_dim = self.data.get_node_dim()\n",
    "\n",
    "        if torch.backends.mps.is_available():\n",
    "            with torch.device(\"mps\"):\n",
    "                self.node_emb = Variable(torch.from_numpy(np.random.uniform(\n",
    "                    -1. / np.sqrt(self.node_dim), 1. / np.sqrt(self.node_dim), (self.node_dim, emb_size))).type(\n",
    "                    FType).to(\"mps\"), requires_grad=True)\n",
    "\n",
    "                self.delta = Variable((torch.zeros(self.node_dim) + 1.).type(FType).to(\"mps\"), requires_grad=True)\n",
    "\n",
    "                self.att_param = Variable(torch.diag(torch.from_numpy(np.random.uniform(\n",
    "                    -1. / np.sqrt(emb_size), 1. / np.sqrt(emb_size), (emb_size,))).type(\n",
    "                    FType).to(\"mps\")), requires_grad=True)\n",
    "        else:\n",
    "            self.node_emb = Variable(torch.from_numpy(np.random.uniform(\n",
    "                -1. / np.sqrt(self.node_dim), 1. / np.sqrt(self.node_dim), (self.node_dim, emb_size))).type(\n",
    "                FType), requires_grad=True)\n",
    "\n",
    "            self.delta = Variable((torch.zeros(self.node_dim) + 1.).type(FType), requires_grad=True)\n",
    "\n",
    "            self.att_param = Variable(torch.diag(torch.from_numpy(np.random.uniform(\n",
    "                -1. / np.sqrt(emb_size), 1. / np.sqrt(emb_size), (emb_size,))).type(\n",
    "                FType)), requires_grad=True)\n",
    "\n",
    "        self.opt = SGD(lr=learning_rate, params=[self.node_emb, self.att_param, self.delta])\n",
    "        self.loss = torch.FloatTensor()\n",
    "\n",
    "    def forward(self, s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask):\n",
    "        batch = s_nodes.size()[0]\n",
    "        s_node_emb = self.node_emb.index_select(0, Variable(s_nodes.view(-1))).view(batch, -1)\n",
    "        t_node_emb = self.node_emb.index_select(0, Variable(t_nodes.view(-1))).view(batch, -1)\n",
    "        h_node_emb = self.node_emb.index_select(0, Variable(h_nodes.view(-1))).view(batch, self.hist_len, -1)\n",
    "\n",
    "        att = softmax(((s_node_emb.unsqueeze(1) - h_node_emb)**2).sum(dim=2).neg(), dim=1)\n",
    "        p_mu = ((s_node_emb - t_node_emb)**2).sum(dim=1).neg()\n",
    "        p_alpha = ((h_node_emb - t_node_emb.unsqueeze(1))**2).sum(dim=2).neg()\n",
    "\n",
    "        delta = self.delta.index_select(0, Variable(s_nodes.view(-1))).unsqueeze(1)\n",
    "        d_time = torch.abs(t_times.unsqueeze(1) - h_times)  # (batch, hist_len)\n",
    "        p_lambda = p_mu + (att * p_alpha * torch.exp(delta * Variable(d_time)) * Variable(h_time_mask)).sum(dim=1)\n",
    "        \n",
    "        n_node_emb = self.node_emb.index_select(0, Variable(n_nodes.view(-1))).view(batch, self.neg_size, -1)\n",
    "\n",
    "        n_mu = ((s_node_emb.unsqueeze(1) - n_node_emb)**2).sum(dim=2).neg()\n",
    "        n_alpha = ((h_node_emb.unsqueeze(2) - n_node_emb.unsqueeze(1))**2).sum(dim=3).neg()\n",
    "\n",
    "\n",
    "        n_lambda = n_mu + (att.unsqueeze(2) * n_alpha * (torch.exp(delta * Variable(d_time)).unsqueeze(2)) * (Variable(h_time_mask).unsqueeze(2))).sum(dim=1)\n",
    "        return p_lambda, n_lambda\n",
    "\n",
    "    def loss_func(self, s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask):\n",
    "        if torch.backends.mps.is_available():\n",
    "            with torch.device(\"mps\"):\n",
    "                p_lambdas, n_lambdas = self.forward(s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask)\n",
    "                loss = -torch.log(p_lambdas.sigmoid() + 1e-6) - torch.log(\n",
    "                    n_lambdas.neg().sigmoid() + 1e-6).sum(dim=1)\n",
    "\n",
    "        else:\n",
    "            p_lambdas, n_lambdas = self.forward(s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times,\n",
    "                                                h_time_mask)\n",
    "            loss = -torch.log(torch.sigmoid(p_lambdas) + 1e-6) - torch.log(\n",
    "                torch.sigmoid(torch.neg(n_lambdas)) + 1e-6).sum(dim=1)\n",
    "        return loss\n",
    "\n",
    "    def update(self, s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask):\n",
    "        if torch.backends.mps.is_available():\n",
    "            with torch.device(\"mps\"):\n",
    "                self.opt.zero_grad()\n",
    "                loss = self.loss_func(s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask)\n",
    "                loss = loss.sum()\n",
    "                self.loss += loss.data\n",
    "                loss.backward()\n",
    "                self.opt.step()\n",
    "        else:\n",
    "            self.opt.zero_grad()\n",
    "            loss = self.loss_func(s_nodes, t_nodes, t_times, n_nodes, h_nodes, h_times, h_time_mask)\n",
    "            loss = loss.sum()\n",
    "            self.loss += loss.data\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.loss = 0.0\n",
    "            loader = DataLoader(self.data, batch_size=self.batch,\n",
    "                                shuffle=True, num_workers=5)\n",
    "            # if epoch % self.save_step == 0 and epoch != 0:\n",
    "            #     #torch.save(self, './model/dnrl-dblp-%d.bin' % epoch)\n",
    "            #     self.save_node_embeddings('./emb/cows_htne_attn_%d.emb' % (epoch))\n",
    "\n",
    "            for i_batch, sample_batched in enumerate(loader):\n",
    "                if i_batch % 100 == 0 and i_batch != 0:\n",
    "                    sys.stdout.write('\\r' + str(i_batch * self.batch) + '\\tloss: ' + str(\n",
    "                        self.loss.cpu().numpy() / (self.batch * i_batch)) + '\\tdelta:' + str(\n",
    "                        self.delta.mean().cpu().data.numpy()))\n",
    "                    sys.stdout.flush()\n",
    "\n",
    "                if torch.backends.mps.is_available():\n",
    "                    with torch.device(\"mps\"):\n",
    "                        self.update(sample_batched['source_node'].type(LType).to(\"mps\"),\n",
    "                                    sample_batched['target_node'].type(LType).to(\"mps\"),\n",
    "                                    sample_batched['target_time'].type(FType).to(\"mps\"),\n",
    "                                    sample_batched['neg_nodes'].type(LType).to(\"mps\"),\n",
    "                                    sample_batched['history_nodes'].type(LType).to(\"mps\"),\n",
    "                                    sample_batched['history_times'].type(FType).to(\"mps\"),\n",
    "                                    sample_batched['history_masks'].type(FType).to(\"mps\"))\n",
    "                else:\n",
    "                    self.update(sample_batched['source_node'].type(LType),\n",
    "                                sample_batched['target_node'].type(LType),\n",
    "                                sample_batched['target_time'].type(FType),\n",
    "                                sample_batched['neg_nodes'].type(LType),\n",
    "                                sample_batched['history_nodes'].type(LType),\n",
    "                                sample_batched['history_times'].type(FType),\n",
    "                                sample_batched['history_masks'].type(FType))\n",
    "\n",
    "            sys.stdout.write('\\repoch ' + str(epoch) + ': avg loss = ' +\n",
    "                                str(self.loss.cpu().numpy() / len(self.data)) + '\\n')\n",
    "            sys.stdout.flush()\n",
    "\n",
    "            self.save_node_embeddings('./emb/cows_htne_attn_%d.emb' % (epoch))\n",
    "\n",
    "    def save_node_embeddings(self, path):\n",
    "        if torch.backends.mps.is_available():\n",
    "            embeddings = self.node_emb.cpu().data.numpy()\n",
    "        else:\n",
    "            embeddings = self.node_emb.data.numpy()\n",
    "        writer = open(path, 'w')\n",
    "        writer.write('%d %d\\n' % (self.node_dim, self.emb_size))\n",
    "        for n_idx in range(self.node_dim):\n",
    "            writer.write(' '.join(str(d) for d in embeddings[n_idx]) + '\\n')\n",
    "\n",
    "        writer.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48\n"
     ]
    }
   ],
   "source": [
    "model = HTNE_a(\n",
    "    file_path=path_cow_data,\n",
    "    emb_size=128,\n",
    "    neg_size=10,\n",
    "    hist_len=2,\n",
    "    directed=False,\n",
    "    epoch_num=35\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=83, pipe_handle=97)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/josephallyndree/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/Users/josephallyndree/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'HTNEDataSet' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[13], line 104\u001b[0m, in \u001b[0;36mHTNE_a.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     98\u001b[0m loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch,\n\u001b[1;32m     99\u001b[0m                     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# if epoch % self.save_step == 0 and epoch != 0:\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m#     #torch.save(self, './model/dnrl-dblp-%d.bin' % epoch)\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m#     self.save_node_embeddings('./emb/cows_htne_attn_%d.emb' % (epoch))\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i_batch, sample_batched \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i_batch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m i_batch \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    106\u001b[0m         sys\u001b[38;5;241m.\u001b[39mstdout\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i_batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mloss: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m    107\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy() \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch \u001b[38;5;241m*\u001b[39m i_batch)) \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124mdelta:\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\n\u001b[1;32m    108\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mnumpy()))\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/torch/utils/data/dataloader.py:491\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/PFR_graph/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PFR_graph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
